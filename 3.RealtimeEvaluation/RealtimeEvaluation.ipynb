{"cells":[{"cell_type":"markdown","id":"19812aa8","metadata":{"id":"19812aa8"},"source":["![](https://drive.google.com/uc?export=view&id=1fefljRA_AjosOAts0bK9fGWYU4QiOvUT)"]},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/gdrive/', force_remount=True)\n","%cd \"/content/gdrive/MyDrive/M2 SETSIS/Deep Learning Son/repertoire_tp/3.RealtimeEvaluation\"\n","!echo $PWD"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2klwGyut3DQa","executionInfo":{"status":"ok","timestamp":1675605410656,"user_tz":-60,"elapsed":27023,"user":{"displayName":"Davy MILLION","userId":"18112015135223475504"}},"outputId":"db98f2fc-61a7-4408-f5fc-6e691f7ebb45"},"id":"2klwGyut3DQa","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","/content/gdrive/MyDrive/M2 SETSIS/Deep Learning Son/repertoire_tp/3.RealtimeEvaluation\n","/content/gdrive/MyDrive/M2 SETSIS/Deep Learning Son/repertoire_tp/3.RealtimeEvaluation\n"]}]},{"cell_type":"markdown","id":"7bca25e3","metadata":{"id":"7bca25e3"},"source":["<h1 align=\"left\" style=\"color:#000051;font-size: 30px\">TP : Classifiez des √©motions vocales avec du deep learning</h1>"]},{"cell_type":"markdown","id":"80d18925","metadata":{"id":"80d18925"},"source":["Pour ce TP, vous impl√©menterez un classifieur √† base de r√©seau de neurones sur des donn√©es combinant les jeux de donn√©es RAVDESS et TESS avec la librairie keras. \n","Repartez de ce notebook Jupyter qui passe en revue chaque partie du TP pour pr√©senter des preuves et une analyse de vos r√©sultats"]},{"cell_type":"markdown","id":"42b26875","metadata":{"id":"42b26875"},"source":["<h1 align=\"left\" style=\"color:#000051;font-size: 25px\">Partie 3 : Evaluation en temps r√©el</h1>"]},{"cell_type":"markdown","id":"46f3618c","metadata":{"id":"46f3618c"},"source":["Une fois le mod√®le entrain√© et √©valu√©, nous allons l'utiliser en temps r√©el pour pr√©dire l'√©motion dans la voix d'un locuteur. Dans cette partie, il est necessaire de disposer d'un microphone microphone USB ou microphone int√©gr√© de votre ordinateur)"]},{"cell_type":"markdown","id":"674ff4fb","metadata":{"id":"674ff4fb"},"source":["<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span>üíæ <strong>A propos des jeux de donn√©es </strong></span></h2>"]},{"cell_type":"markdown","id":"98b871c1","metadata":{"id":"98b871c1"},"source":["Pour ce TP, nous utiliserons 2 jeux de donn√©es\n","\n","- **RAVDESS : Ryerson Audio-Visual Database of Emotional Speech and Song**\n","https://zenodo.org/record/1188976#.X4sE0tDXKUl\n","  - RAVDESS a √©t√© enregistr√©e avec 24 acteurs professionnels (12 femmes, 12 hommes), pronon√ßant deux phrases lexicalement identiques avec un accent nord-am√©ricain neutre. Chaque phrase est prononc√©e avec deux niveaux d'intensit√© √©motionnelle (normal, fort).\n","  - **1440 fichiers** = 24 acteurs x 60 fichiers audio par acteur\n","  - **8 √©motions** (neutre, calme, joie, tristesse, col√®re, peur, d√©gout, surprise).\n","\n","\n","\n","- **TESS : Toronto Emotional Speech Set**\n","https://tspace.library.utoronto.ca/handle/1807/24487\n","  - Ces donn√©es ont √©t√© enregistr√©es par le Northwestern University Auditory. Un ensemble de 200 mots cibles ont √©t√© prononc√©s dans la phrase \"Dites le mot _____\" par deux actrices (√¢g√©es de 26 et 64 ans) et des enregistrements ont √©t√© r√©alis√©s lorsque ces phrases ont √©t√© prononc√©es avec chacune des sept √©motions d√©crites ci-dessous.\n","Les deux actrices ont √©t√© recrut√©es dans la r√©gion de Toronto. Les deux actrices parlent l'anglais comme premi√®re langue, ont fait des √©tudes universitaires et ont une formation musicale.\n","  - **2800 fichiers** = 2 acteurs x 200 phrases x 7 √©motions\n","  - **7 √©motions** (neutre, joie, tristesse, col√®re, peur, d√©go√ªt, surprise)('calme' ne fait pas partie de cette BD) "]},{"cell_type":"markdown","id":"dd9a7d87","metadata":{"id":"dd9a7d87"},"source":["Pour ce TP, pour des consid√©rations de volume de donn√©es, nous n'avons retenu que 4 des √©motions:\n","- **neutre**\n","- **joie**\n","- **tristesse**\n","- **col√®re**"]},{"cell_type":"markdown","id":"0f7879d2","metadata":{"id":"0f7879d2"},"source":["<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span>üì• <strong>1. Import des librairies </strong></span></h2>"]},{"cell_type":"markdown","id":"4df36d90","metadata":{"id":"4df36d90"},"source":["Si vous avez besoin d'installer des libraires Python pour ce TP, d√©commentez et ex√©cutez la cellule ci-dessous"]},{"cell_type":"code","execution_count":null,"id":"c73c580e","metadata":{"id":"c73c580e"},"outputs":[],"source":["#!pip3 install numpy==1.18.5\n","#!pip3 install pydub\n","#!pip3 install librosa\n","#!pip3 install noisereduce\n","#!pip3 install matplotlib\n","#!pip3 install IPython\n","#!pip3 install tensorflow\n","#!pip3 install scikit-learn\n","#!pip3 install scipy\n","#!pip3 install pyaudio\n","#!pip3 install wave"]},{"cell_type":"code","execution_count":null,"id":"a8d2127c","metadata":{"id":"a8d2127c"},"outputs":[],"source":["# Librairies de la biblioth√®que standard Python\n","import os\n","import random\n","import sys\n","import time\n","\n","# Librairies de calcul num√©rique\n","import numpy as np\n","\n","# Librairies de traitement audio\n","from pydub import AudioSegment, effects\n","import librosa\n","import soundfile as sf\n","from scipy.io import wavfile\n","from scipy.io.wavfile import write\n","import noisereduce as nr\n","from librosa import display   \n","import pyaudio\n","import wave\n","\n","# Librairies de data-visualization\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd \n","import seaborn as sns\n","%matplotlib inline\n","\n","# Librairies de machine learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import normalize\n","import tensorflow as tf\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","session = tf.Session(config=config)\n","import time\n","import tensorflow\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.models import Model, load_model, Sequential\n","from tensorflow.keras.layers import Permute, Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Bidirectional, Conv2D\n","from tensorflow.keras.layers import GRU, CuDNNGRU, LSTM, Bidirectional, BatchNormalization, Reshape, MaxPooling2D, Flatten\n","from tensorflow.keras.utils import normalize, plot_model\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"]},{"cell_type":"markdown","id":"28216bbb","metadata":{"id":"28216bbb"},"source":["<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span>üì• <strong>2. Import des fonctions de pr√©-traitements </strong></span></h2>"]},{"cell_type":"markdown","id":"0f4400e5","metadata":{"id":"0f4400e5"},"source":["Importez ici la fonction **\"process_data()\"** permettant de r√©aliser la chaine de pr√©-traitement sur un fichier audio"]},{"cell_type":"code","execution_count":null,"id":"1eb92a88","metadata":{"id":"1eb92a88"},"outputs":[],"source":["from preprocessing import process_data"]},{"cell_type":"code","execution_count":null,"id":"b88e7a2b","metadata":{"id":"b88e7a2b"},"outputs":[],"source":["EMOTION_DICT = {0:\"neutre\", 1:\"joie\", 2:\"tristesse\", 3:\"colere\"}"]},{"cell_type":"code","execution_count":null,"id":"2e64fce4","metadata":{"id":"2e64fce4"},"outputs":[],"source":["FILE = \"../1.DataPreprocessing/tmp.wav\"\n","sampling_rate = librosa.core.get_samplerate(FILE)\n","\n","process_data(audio_filename=FILE, \n","             hop_length=512, \n","             frame_length=2048, \n","             sampling_rate=sampling_rate)"]},{"cell_type":"markdown","id":"f26ee256","metadata":{"id":"f26ee256"},"source":["<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span>üß† <strong>3. Chargement du mod√®le de deep learning entrain√©</strong></span></h2>"]},{"cell_type":"markdown","id":"0a2425df","metadata":{"id":"0a2425df"},"source":["Nous allons utiliser le mod√®le que nous avons entrain√© et sauvegard√© pr√©c√©demment. Nous allons donc charger ce mod√®le avec **load_model** de Keras"]},{"cell_type":"code","execution_count":null,"id":"31b1343b","metadata":{"id":"31b1343b"},"outputs":[],"source":["MODEL_FILE = \"../2.ModelTraining/model.h5\"\n","from tensorflow.keras.models import load_model\n","\n","model = load_model(MODEL_FILE)"]},{"cell_type":"markdown","id":"247dcf3a","metadata":{"id":"247dcf3a"},"source":["<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span>üîé <strong>4. Fonction de pr√©diction d'une √©motion √† partir d'un fichier audio de 96000 √©chantillons</strong></span></h2>"]},{"cell_type":"markdown","id":"05daf6c6","metadata":{"id":"05daf6c6"},"source":["Lors de l'√©tape de pr√©-traitement, nous avons √©galiser tous les fichiers sonores afin qu'ils soient tous de longueur 96000 √©chantillons. Cette √©galisation permet d'avoir les m√™mes dimensions de donn√©es en entr√©e du mod√®le. \n","\n","Nous allons ici d√©velopper une fonction qui permet de bout-en-bout de prendre en entr√©e des fichiers audio de 96000 √©chantillons et pr√©dire en sortie l'√©motion exprim√©e dans le fichier audio."]},{"cell_type":"markdown","id":"e6257550","metadata":{"id":"e6257550"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Ecrivez le code dans la fonction  <strong>detect_emotion_from_2sfile</strong> permettant de d√©tecter l'√©motion dans un fichier audio de 2 secondes environ (96000 √©chantillons) √† partir d'un fichier audio en entr√©e, puis tester cette fonction sur le fichier exemple <strong>FILE</strong></span></p>"]},{"cell_type":"markdown","id":"765947a3","metadata":{"id":"765947a3"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Des emojis sont disponibles dans le dossier <strong>\"assets\"</strong>. </span></p>"]},{"cell_type":"code","execution_count":null,"id":"d2b03b79","metadata":{"id":"d2b03b79"},"outputs":[],"source":["HOP_LENGTH = 512\n","FRAME_LENGTH = 2048"]},{"cell_type":"code","execution_count":null,"id":"2b8a77e8","metadata":{"id":"2b8a77e8"},"outputs":[],"source":["def detect_emotion_from_2sfile(audio_filename, display_emoji=True, display_chart=True):\n","    \n","    '''\n","    Fonction permettant de d√©tecter l'√©motion dans un fichier audio de 2 secondes environ (96000 √©chantillons)\n","\n","            Parameters:\n","                    audio_filename (str): Chaine de caract√®re correspondant au chemin d'acc√®s au fichier audio\n","                    display_emoji (bool) : Bool√©en indiquant d'afficher ou non l'√©moji correspondant √† l'√©motion d√©tect√©e\n","                    display_chart (bool): Bool√©en indiquant d'afficher ou non un graphique de r√©partition de probabilit√© d'appartenance aux 4 √©motions du jeu de donnn√©es\n","\n","\n","            Returns:\n","                    preprocessed_array (Numpy array): Matrice de nombres repr√©sentant les √©chantillons audio apr√®s pr√©-traitement (normalisation, ...)\n","                    emotion_name (str): Chaine de caract√®res indiquant le nom de l'√©motion d√©tect√©e\n","\n","\n","    '''\n","    \n","    sampling_rate = librosa.core.get_samplerate(audio_filename)\n","\n","    preprocessed_array, feature = process_data(audio_filename = audio_filename, \n","                                               hop_length     = HOP_LENGTH, \n","                                               frame_length   = FRAME_LENGTH, \n","                                               sampling_rate  = sampling_rate)\n","    \n","    feature = normalize(feature)\n","    \n","    feature = np.expand_dims(feature, axis=0)\n","    \n","    predictions_proba = model.predict_proba(feature)\n","    \n","    emotion_name = EMOTION_DICT[np.argmax(predictions_proba)]\n","    if display_emoji:\n","        ipd.display(ipd.Image(\"./assets/{}.png\".format(emotion_name),width=30, height=30))\n","    \n","    if display_chart:    \n","        fig = plt.figure(figsize = (10, 2))\n","        plt.bar(list(EMOTION_DICT.values()), predictions_proba.tolist()[0], color = 'darkturquoise')\n","        plt.ylabel(\"Probabilty (%)\")\n","        plt.show()\n","    \n","    return preprocessed_array, emotion_name"]},{"cell_type":"code","execution_count":null,"id":"3425b9fc","metadata":{"id":"3425b9fc"},"outputs":[],"source":["FILE = \"../1.DataPreprocessing/dataset/tess/OAF_yes_angry.wav\"\n"]},{"cell_type":"markdown","id":"468a41a4","metadata":{"id":"468a41a4"},"source":["<h2 style=\"text-align: left; color:#20a08d; font-size: 25px\"><span>üé§ <strong>5. Fonction d'acquisition des donn√©es audio en temps r√©el √† partir d'un microphone</strong></span></h2>"]},{"cell_type":"markdown","id":"0bdcb371","metadata":{"id":"0bdcb371"},"source":["Le but de cette partie consiste √† d√©velopper une fonction qui capture en temps r√©el le flux audio d'un microphone puis enregistrer s√©quentiellement ce flux en fichier audio wav temporaires de 96000 √©chantillons (2 secondes avec fr√©quence d'√©chantillonnage √† 48 KHz)"]},{"cell_type":"markdown","id":"5235fc69","metadata":{"id":"5235fc69"},"source":["<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span>üé§ <strong>5.1 D√©couverte des p√©riph√©riques audio disponibles </strong></span></h3>"]},{"cell_type":"markdown","id":"97f3369e","metadata":{"id":"97f3369e"},"source":["Avant d'utiliser un microphone comme p√©riph√©rique d'entr√©e, visualisons l'ensemble des p√©riph√©riques conn√©ct√©s √† l'ordinateur ainsi que quelques une de leurs caract√©ristiques"]},{"cell_type":"markdown","id":"e076a891","metadata":{"id":"e076a891"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Executez la fonction  <strong>discover_audio_devices</strong> permettant d'afficher les p√©riph√©riques audio mono ou st√©r√©o connect√©s √† l'ordinateur. R√©p√©rez l'identifiant correspondant au p√©riph√©rique que vous voulez utiliser </span></p>"]},{"cell_type":"code","execution_count":null,"id":"08776741","metadata":{"id":"08776741"},"outputs":[],"source":["def discover_audio_devices():\n","    \n","    '''\n","    Fonction permettant d'afficher les p√©riph√©riques audio mono ou st√©r√©o connect√©s √† l'ordinateur\n","\n","            Parameters:\n","                    Rien : affiche la liste des p√©ripƒ•√©riques\n","                    \n","            Returns:\n","                    Rien : affiche la liste des p√©ripƒ•√©riques\n","\n","\n","    '''\n","        \n","    p = pyaudio.PyAudio()\n","    info = p.get_host_api_info_by_index(0)\n","    numdevices = info.get('deviceCount')\n","    device_index = None\n","\n","    for i in range(0, numdevices):\n","        channels = p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')\n","        if  channels == 1 or channels == 2:\n","            DeviceName = p.get_device_info_by_host_api_device_index(0, i).get('name')\n","            print(\"ID : {} ==> {} ==> Nombre de canaux = {}\".format(i, DeviceName, channels))\n"," \n","            supported_samplerates = []\n","            SAMPLERATES = [16000, 32000, 44100, 48000]\n","            devinfo = p.get_device_info_by_index(i)\n","\n","\n","            for fs in SAMPLERATES:\n","                try:\n","                    p.is_format_supported(fs,  # Sample rate\n","                                          input_device=devinfo[\"index\"],\n","                                          input_channels=devinfo['maxInputChannels'],\n","                                          input_format=pyaudio.paInt32)\n","                except Exception as e:\n","                    pass\n","                    #print(fs, e)\n","                else:\n","                    supported_samplerates.append(fs)\n","\n","            print(\"Fr√©quence d'√©chantillonnage support√©s = \", supported_samplerates)"]},{"cell_type":"code","execution_count":null,"id":"536795db","metadata":{"id":"536795db"},"outputs":[],"source":["discover_audio_devices()"]},{"cell_type":"markdown","id":"ca8f5d4e","metadata":{"id":"ca8f5d4e"},"source":["<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span>üé§ <strong>5.2 D√©veloppement de la fonction d'acquisition temps r√©el </strong></span></h3>"]},{"cell_type":"markdown","id":"9a04bdda","metadata":{"id":"9a04bdda"},"source":["Dans cette partie, nous allons exploiter le p√©riph√©rique audio d√©tect√© pr√©c√©demment pour enregsitrer en boucle un fichier temporaire \"tmp.wav\" de 2 secondes √† 48000 Hz (soit 96000 √©chantillons).\n","\n","Pour faire l'acquisition des 96000 √©chantillons, nous allons utiliser la librairie PyAudio qui permet de cr√©er des objets \"stream\". \n","\n","L'acquisition est it√©ratif, c'est-√†-dire que les √©chantillons sont acquis lot par lot. Les lots s'appellent des **\"chunks\"**"]},{"cell_type":"markdown","id":"3ebeec1b","metadata":{"id":"3ebeec1b"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Ecrivez le code la fonction  <strong>record_2swav_from_stream</strong> d'enregistrer des fichiers audio de 2 secondes environ (96000 √©chantillons) √† partir d'un p√©riph√©rique audio d'entr√©e. Testez ensuite la fonction sur un de vos p√©riph√©riques audio d'entr√©e. </span></p>"]},{"cell_type":"markdown","id":"a3b84638","metadata":{"id":"a3b84638"},"source":["<p style=\"text-align: left; font-size: 16px; color:#ec8f1a\"><span>üìö  \n","PyAudio : </span> <a href=\"http://people.csail.mit.edu/hubert/pyaudio/#examples\">http://people.csail.mit.edu/hubert/pyaudio/#examples"]},{"cell_type":"markdown","id":"58898a1a","metadata":{"id":"58898a1a"},"source":["<p style=\"text-align: left; font-size: 16px; color:#ec8f1a\"><span>üìö  \n","Reading realtime audio data into numpy array\n"," : </span> <a href=\"https://stackify.dev/200442-reading-realtime-audio-data-into-numpy-array\">https://stackify.dev/200442-reading-realtime-audio-data-into-numpy-array"]},{"cell_type":"markdown","id":"78fa1fbd","metadata":{"id":"78fa1fbd"},"source":["<p style=\"text-align: left; font-size: 16px; color:#ec8f1a\"><span>üìö  \n","scipy.io.wavfile.write\n"," : </span> <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html"]},{"cell_type":"code","execution_count":null,"id":"829d3f2f","metadata":{"id":"829d3f2f"},"outputs":[],"source":["# Initialize variables\n","SAMPLING_RATE = 48000\n","CHUNK = 512\n","RECORD_SECONDS = 2\n","\n","FORMAT = pyaudio.paInt32\n","CHANNELS = 1\n","\n","# Open an input channel\n","p = pyaudio.PyAudio()"]},{"cell_type":"code","execution_count":null,"id":"6f545705","metadata":{"id":"6f545705"},"outputs":[],"source":["def record_2swav_from_stream(stream, chunk, sampling_rate, record_seconds, save_wav=True):\n","    \n","    '''\n","    Fonction permettant d'enregistrer des fichiers audio de 2 secondes environ (96000 √©chantillons) √† partir d'un p√©riph√©rique audio d'entr√©e\n","\n","            Parameters:\n","                    stream (Objet Pyaudio stream): Objet Pyaudio stream correspondant au p√©riph√©rique audio √† utiliser\n","                    chunk (int) : Nombre entier repr√©sentant la taille du groupe d'√©chantillon √† acqu√©rir √†  chaque it√©ration\n","                    record_seconds (int) : Nombre entier la dur√©e en secondes du fichier audio final\n","                    sampling_rate (int) : Nombre entier repr√©sentant la fr√©quence d'√©chantillonnage du fichier audio\n","\n","\n","            Returns:\n","                    full_frames (Numpy array): Matrice de nombres repr√©sentant les √©chantillons audio du fichier audio final de 2 secondes (96000 √©chantillons)\n","\n","\n","    '''\n","    \n","    return full_frames"]},{"cell_type":"code","execution_count":null,"id":"c0dea872","metadata":{"id":"c0dea872"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e0f8310f","metadata":{"id":"e0f8310f"},"source":["<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span>üé§ <strong>5.3 D√©veloppement d'une fonction de d√©tection de silence </strong></span></h3>"]},{"cell_type":"markdown","id":"fbd0ea77","metadata":{"id":"fbd0ea77"},"source":["Nous d√©veloppons ici une fonction de d√©tection de silence. Le but de cette fonction est de faire l'acquisition en boucle de fichiers audio, puis lorsque pendant 5 secondes le niveau sonore est inf√©rieur √† un seuil, arr√™ter l'acquisition."]},{"cell_type":"markdown","id":"67626d77","metadata":{"id":"67626d77"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Ex√©cutez le code ci-dessous pour tester la fonctionnalit√© de d√©tection de silence </span></p>"]},{"cell_type":"code","execution_count":null,"id":"5da6cdab","metadata":{"id":"5da6cdab"},"outputs":[],"source":["def is_silent(data):\n","    # Returns 'True' if below the 'silent' threshold\n","    return max(data) < 100"]},{"cell_type":"code","execution_count":null,"id":"f0b03cbc","metadata":{"id":"f0b03cbc"},"outputs":[],"source":["SILENT_SECONDS = 5"]},{"cell_type":"code","execution_count":null,"id":"50486bac","metadata":{"id":"50486bac"},"outputs":[],"source":["SAMPLES_SILENT = SILENT_SECONDS*SAMPLING_RATE\n","silent_check_array = np.zeros(SAMPLES_SILENT)\n","\n","while True:\n","    t0 = time.time()\n","    data_array = record_2swav_from_stream(stream=stream, \n","                                        chunk=CHUNK, \n","                                        sampling_rate=SAMPLING_RATE, \n","                                        record_seconds=RECORD_SECONDS,\n","                                        save_wav=False)\n","    \n","    librosa.display.waveplot(data_array.astype(float), SAMPLING_RATE)\n","    plt.show()\n","    \n","    print(time.time()-t0)\n","    silent_check_array = np.append(silent_check_array, data_array)\n","    silent_check_array = silent_check_array[-SAMPLES_SILENT:]\n","    print(\"MAX => \", np.max(silent_check_array))\n","    \n","    if is_silent(data=silent_check_array):\n","        print(\"[INFO] : Silence d√©tect√© ... Arr√™t de l'acquisition audio !\")\n","        break"]},{"cell_type":"markdown","id":"50b8e8c2","metadata":{"id":"50b8e8c2"},"source":["<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span>üóÇÔ∏è <strong>6 D√©veloppement de la fonction de d√©tection en temps r√©el √† partir d'un fichier audio</strong></span></h3>"]},{"cell_type":"markdown","id":"0ec4fa35","metadata":{"id":"0ec4fa35"},"source":["Le but ici est d'analyser les √©motions exprim√©es dans un fichier audio √† partir des fonctions d√©velopp√©es pr√©c√©demment.\n","\n","Le proc√©d√© consiste √† charger la totalit√© des √©chantillons du fichier audio, puis d√©couper par lot de 96000 √©chantillons, enregistrer dans un fichier temporaire, et ex√©cuter la d√©tection d'√©motion sur le fichier temporaire"]},{"cell_type":"code","execution_count":null,"id":"e3b2b5f8","metadata":{"id":"e3b2b5f8"},"outputs":[],"source":["TEST_AUDIO_FILE = \"./greta_thunberg.wav\"\n","MAX_SAMPLES = 96000"]},{"cell_type":"markdown","id":"02b56cd6","metadata":{"id":"02b56cd6"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Ecrivez le code permettant de d√©tecter les √©motions pr√©sente dans le fichier <strong>TEST_AUDIO_FILE</strong> </span></p>"]},{"cell_type":"markdown","id":"edc83f45","metadata":{"id":"edc83f45"},"source":["<p style=\"text-align: left; font-size: 16px; color:#ec8f1a\"><span>üìö  \n","librosa.load : </span> <a href=\"https://librosa.org/doc/main/generated/librosa.load.html\">https://librosa.org/doc/main/generated/librosa.load.html"]},{"cell_type":"markdown","id":"a828aeb5","metadata":{"id":"a828aeb5"},"source":["<p style=\"text-align: left; font-size: 16px; color:#ec8f1a\"><span>üìö  \n","scipy.io.wavfile.write\n"," : </span> <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html"]},{"cell_type":"code","execution_count":null,"id":"2a10b699","metadata":{"id":"2a10b699"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ce583854","metadata":{"id":"ce583854"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"1f1082b2","metadata":{"id":"1f1082b2"},"source":["<h3 style=\"text-align: left; color:#20a08d; font-size: 20px\"><span>üé§ <strong>5.5 D√©veloppement de la fonction de d√©tection en temps r√©el √† partir d'un microphone</strong></span></h3>"]},{"cell_type":"markdown","id":"2c5580d1","metadata":{"id":"2c5580d1"},"source":["Le but ici est d'analyser les √©motions exprim√©es directement dans un flux audio en provenance d'un microphone.\n","\n","Le proc√©d√© consiste √† faire l'acquisition des √©chantillons audio chunk par chunk, puis chaque fois que 96000 √©chantillons sont acquis, ils sont enregistr√©s sous forme d'un fichier temporaire. Le fichier temporaire est ensuite analys√© en utilisant la fonction de d√©tection d'√©motion."]},{"cell_type":"markdown","id":"167d5b2f","metadata":{"id":"167d5b2f"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Ecrivez le code permettant de d√©tecter les √©motions dans le flux audio en provenance du microphone de votre ordinateur. </span></p>"]},{"cell_type":"markdown","id":"899b4eb5","metadata":{"id":"899b4eb5"},"source":["<p style=\"text-align: left; font-size: 16px; color:#131fcf\"><span>üñ•Ô∏è  Pour aller plus loin, utilisez la fonction de d√©tection de silence pour arr√™ter l'acquisition audio lorsqu'il y a eu un silence pendant 5 secondes ou plus </span></p>"]},{"cell_type":"code","execution_count":null,"id":"6b1a6b88","metadata":{"id":"6b1a6b88"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}